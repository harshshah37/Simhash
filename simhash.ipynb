{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2bf4bf-dd06-4476-9713-2f700ad91428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import hamming, euclidean\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d658c96c-93ef-4c48-9c41-d52c9a918435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning for extraction of meaningful words\n",
    "#Using TF-IDF over Count Vectorizer because it also takes into account the importance of words\n",
    "\n",
    "def extract_features(documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print(features, \"\\n\")\n",
    "    \n",
    "    return X.toarray(), features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a4e549-4834-4799-9766-64c98a8d9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating simhash fingerprints\n",
    "\n",
    "def simhash_fingerprint(features, weights, f=64):\n",
    "    V = np.zeros(f)\n",
    "    for feature, weight in zip(features, weights):\n",
    "        #Generating 64-bit fingerprints\n",
    "        h = ctypes.c_size_t(hash(feature)).value \n",
    "        b = bin(h)[2:] \n",
    "        z = str(b)\n",
    "\n",
    "        #Ensuring lenghth of every fingerprint is 64 bit\n",
    "        if len(z) != 64:\n",
    "            for i in range(64):\n",
    "                z = '0' + z\n",
    "                if len(z) == 64: break\n",
    "\n",
    "        #Multiplying each bit by its positive weight if it is 1 and negative weight if it is 0\n",
    "        for i in range(f):\n",
    "            if z[i] == '1':\n",
    "                V[i] += weight\n",
    "            elif z[i] == '0':\n",
    "                V[i] -= weight\n",
    "                \n",
    "    #Generating the final simhash fingerprint for every component in the extracted corpus\n",
    "    fingerprint = ''.join('1' if v >= 0 else '0' for v in V)\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d64dd07-e069-44f8-b5ec-78cb0a8789b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(fingerprint1, fingerprint2):\n",
    "    return round((hamming(list(fingerprint1), list(fingerprint2))), 3)\n",
    "\n",
    "def euclidean_distance(counts1, counts2):\n",
    "    return round((euclidean(counts1, counts2)), 3)\n",
    "\n",
    "def compute_pearson_correlation(distances1, distances2):\n",
    "    return round((pearsonr(distances1, distances2)[0]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8d41b4-834b-4bfb-945a-3ef077cac934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '19' '2021' '668' '9bhydkxz8u' 'ability' 'actual' 'admitted'\n",
      " 'advantage' 'aff' 'africa' 'airway' 'allisonpearson' 'anothe' 'antiviral'\n",
      " 'application' 'aren' 'asterhealthcare' 'babies' 'breakthrough' 'cases'\n",
      " 'caused' 'child' 'cihr' 'cihr_irsc' 'closing' 'continue' 'corrected'\n",
      " 'course' 'covid' 'covid19' 'cross' 'data' 'deis85208721' 'delta' 'did'\n",
      " 'die' 'disruptions' 'drcchambers' 'drmroz' 'edfuw5qjah' 'effect'\n",
      " 'enemyinastate' 'enhance' 'erictopol' 'evade' 'evades' 'extending' 'eyes'\n",
      " 'fail' 'far' 'freethinkfacts' 'going' 'hav' 'high' 'https' 'hyper'\n",
      " 'immune' 'immunity' 'impact' 'inasmuch' 'induced' 'infection'\n",
      " 'ivermectin' 'japan' 'kowa' 'light' 'load' 'london' 'lsoril' 'manifest'\n",
      " 'maxdkozlov' 'mdfacep' 'melulater' 'milde' 'mkky24weqv' 'mutations'\n",
      " 'nabs' 'november' 'older' 'omicron' 'people' 'peterandann' 'pompey1977'\n",
      " 'prevent' 'produced' 'protection' 'quicker' 'recent' 'registration'\n",
      " 'reinfection' 'related' 'reported' 'research' 'results' 'review' 'rt'\n",
      " 'says' 'sealsoftheend' 'showed' 'shown' 'south' 'spreads'\n",
      " 'stevebennett15' 'studies' 'sufficient' 'suggest' 'surge'\n",
      " 'svictor70973566' 'taken' 'thinks' 'toddlers' 'transmissible' 'upper'\n",
      " 'vaccine' 'vaccines' 'variant' 'veoqyz5x6f' 'versa' 'vice' 'viral' 'want'\n",
      " 'wasohope' 'widespread' 'wildantlers' 'working' 'yaneerbaryam'] \n",
      "\n",
      "Hamming Distances: \n",
      " [0.5, 0.484, 0.5, 0.125, 0.516, 0.5, 0.5, 0.484, 0.5, 0.578, 0.469, 0.109, 0.469, 0.578, 0.359, 0.406, 0.469, 0.547, 0.531, 0.375, 0.422, 0.406, 0.516, 0.594, 0.484, 0.281, 0.609, 0.422, 0.484, 0.406, 0.453, 0.422, 0.406, 0.453, 0.469, 0.516, 0.5, 0.391, 0.531, 0.438, 0.516, 0.5, 0.562, 0.516, 0.406, 0.516, 0.594, 0.484, 0.5, 0.422, 0.484, 0.5, 0.469, 0.516, 0.5, 0.484, 0.5, 0.203, 0.469, 0.547, 0.422, 0.328, 0.438, 0.547, 0.406, 0.422, 0.562, 0.516, 0.5, 0.5, 0.391, 0.531, 0.547, 0.562, 0.516, 0.531, 0.422, 0.422, 0.469, 0.453, 0.594, 0.547, 0.406, 0.516, 0.609, 0.594, 0.453, 0.406, 0.484, 0.562, 0.453, 0.594, 0.453, 0.531, 0.578, 0.578, 0.531, 0.422, 0.406, 0.453, 0.656, 0.484, 0.484, 0.594, 0.516] \n",
      "\n",
      "Euclidean Distances: \n",
      " [1.404, 1.395, 1.392, 0.529, 1.379, 1.393, 1.394, 1.321, 1.395, 1.4, 1.398, 0.529, 1.405, 1.392, 1.407, 1.406, 1.404, 1.395, 1.406, 1.332, 1.407, 1.407, 1.367, 1.408, 1.404, 0.817, 1.406, 1.397, 1.395, 1.388, 1.398, 1.364, 1.4, 1.32, 1.403, 1.402, 1.395, 1.408, 1.398, 1.392, 1.384, 1.305, 1.397, 1.398, 1.397, 1.401, 1.4, 1.392, 1.406, 1.304, 1.379, 1.393, 1.394, 1.321, 1.395, 1.4, 1.398, 0.529, 1.405, 1.392, 1.385, 1.387, 1.389, 1.331, 1.334, 1.392, 1.379, 1.397, 1.322, 1.397, 1.398, 1.398, 1.402, 1.4, 1.393, 1.407, 1.249, 1.399, 1.363, 1.348, 1.401, 1.394, 1.341, 1.397, 1.4, 1.403, 1.402, 1.321, 1.408, 1.398, 1.403, 1.402, 1.395, 1.407, 1.398, 1.405, 1.4, 1.372, 1.402, 1.398, 1.408, 1.4, 1.405, 1.392, 1.407] \n",
      "\n",
      "Pearson Correlation: 0.716\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Using the corpus from the sample code provided in the question for testing\n",
    "    #corpus: 15 tweets using Omicron as the query\n",
    "    \n",
    "    documents = [\"DrCChambers RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19,\\\n",
    "    CIHR is extending the registration and application de…\",\n",
    "    \"sealsoftheend Japan's Kowa says Ivermectin showed 'antiviral effect' against Omicron https://t.co/mKKY24WeQV\",\n",
    "    \"SVictor70973566 RT @EricTopol: Why is Omicron so hyper-transmissible? It's not related to high viral load in\\\n",
    "    the upper airway, as shown by 2 recent studies…\",\n",
    "    \"freethinkfacts RT @yaneerbaryam: Actual cases of reinfection by Omicron are so widespread they are manifest to\\\n",
    "    anyone who is not closing their eyes: 10/…\",\n",
    "    \"lsoril RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19, CIHR is extending\\\n",
    "    the registration and application de…\",\n",
    "    \"pompey1977 RT @AllisonPearson: How can people not get it? Omicron’s advantage over Delta is it evades the vaccine.\\\n",
    "    Everyone is going to get Omicron…\",\n",
    "    \"freethinkfacts RT @yaneerbaryam: Taken together, our results suggest that Omicron-induced immunity may not be\\\n",
    "    sufficient to prevent infection from anothe…\",\n",
    "    \"SteveBennett15 RT @EricTopol: Anyone who thinks that vaccines aren't working against Omicron might want to review\\\n",
    "    the data https://t.co/9bHYdKxz8u https:/…\",\n",
    "    \"wasohope RT @ASTERHealthcare: Omicron covid-19 variant was reported from South Africa on November 2021.\\\n",
    "    This variant has had many mutations that aff…\",\n",
    "    \"SVictor70973566 RT @MdFacep: @EricTopol @maxdkozlov Omicron's impact is in its ability to evade our immune system:\\\n",
    "    Our 'older' vaccine produced NABS fail…\",\n",
    "    \"Wildantlers @melulater Of course it did, Omicron spreads far quicker. But as a % of people who die from omicron\\\n",
    "    it is far milde… https://t.co/eDFuW5qjAH\",\n",
    "    \"peterandann RT @EnemyInAState: Omicron, London: Babies and toddlers continue to surge, and 1 in 9 admitted\\\n",
    "    is a child. Over 668 babies and toddlers hav…\",\n",
    "    \"DrMroz RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19, CIHR is extending\\\n",
    "    the registration and application de…\",\n",
    "    \"Deis85208721 CORRECTED-Japan's Kowa says ivermectin showed 'antiviral effect' against Omicron in research\\\n",
    "    https://t.co/VEoQyz5x6F\",\n",
    "    \"freethinkfacts RT @yaneerbaryam: Thus, breakthrough infection from Omicron may enhance cross-protection\\\n",
    "    against Delta, and vice-versa, [only] inasmuch as…\"]\n",
    "\n",
    "    \n",
    "    counts, features = extract_features(documents)\n",
    "    \n",
    "    fingerprints = [simhash_fingerprint(features, counts[i]) for i in range(len(documents))]\n",
    "\n",
    "    #Calculating hamming and euclidean distances between every element in the extracted corpus\n",
    "    hamming_distances = []\n",
    "    euclidean_distances = []\n",
    "    for i in range(len(documents)):\n",
    "        for j in range(i+1, len(documents)):\n",
    "            hamming_dist = hamming_distance(fingerprints[i], fingerprints[j])\n",
    "            euclidean_dist = euclidean_distance(counts[i], counts[j])\n",
    "            hamming_distances.append(hamming_dist)\n",
    "            euclidean_distances.append(euclidean_dist)\n",
    "    \n",
    "    print(\"Hamming Distances: \\n\", hamming_distances, '\\n')\n",
    "    print(\"Euclidean Distances: \\n\", euclidean_distances, '\\n')\n",
    "    \n",
    "    correlation = compute_pearson_correlation(hamming_distances, euclidean_distances)\n",
    "    print(\"Pearson Correlation:\", correlation)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
